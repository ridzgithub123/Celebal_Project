{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62cfc71c-7a3f-4e32-87bf-1afab6fbbf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content snippet from 20newsgroups.data.html:\n",
      "<html><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /><meta http-equiv=\"Content-Style-Type\" content=\"text/css\" /><meta name=\"generator\" content=\"Aspose.Words for .NET 24.2.0\" /><title>20 Newsgroups</title><style type=\"text/css\">body { font-family:'Times New Roman'; font-size:12pt }h1, h2, h3, h4, h5, h6, p { margin:0pt }h1 { margin-top:12pt; margin-bottom:0pt; page-break-inside:avoid; page-break-after:avoid; font-family:'Times New Roman'; font-size:24pt; font-weight:bold; font-style:normal; color:#2f5496 }h2 { margin-top:2pt; margin-bottom:0pt; page-break-inside:avoid; page-break-after:avoid; font-family:'Times New Roman'; font-size:18pt; font-weight:bold; font-style:normal; color:#2f5496 }h3 { margin-top:2pt; margin-bottom:0pt; page-break-inside:avoid; page-break-after:avoid; font-family:'Times New Roman'; font-size:14pt; font-weight:bold; font-style:normal; color:#1f3763 }h4 { margin-top:2pt; margin-bottom:0pt; page-break-inside:avoid; page-break-after:avoi\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "html_file_paths = ['20newsgroups.data.html']\n",
    "\n",
    "# Function to read and print a snippet of the HTML file\n",
    "def read_html_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "\n",
    "# Print a snippet of each HTML file\n",
    "for file_path in html_file_paths:\n",
    "    content = read_html_file(file_path)\n",
    "    print(f\"Content snippet from {file_path}:\")\n",
    "    print(content[:1000])  # Print the first 1000 characters for inspection\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8add50-e54e-45e3-aeb9-2727201c2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = ' '.join([p.get_text() for p in soup.find_all('p')])  # Adjust the tag if necessary\n",
    "        return text\n",
    "\n",
    "documents = []\n",
    "for file_path in html_file_paths:\n",
    "    documents.append(extract_text_from_html(file_path))\n",
    "\n",
    "# Combine all extracted documents into one list\n",
    "documents = [doc for sublist in documents for doc in sublist.split('\\n') if doc.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46f6dc7-dee0-46a0-986a-db5715dbf822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the stemmer and stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize and stem the text\n",
    "    words = text.lower().split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a416518-1a64-4765-9215-756ad5424d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.7.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284bb80d-ebd1-447e-ba51-51749c13e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed documents: 1\n",
      "Document-term matrix shape: (1, 119)\n",
      "Topic 0:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 1:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 2:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 3:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 4:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 5:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 6:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 7:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 8:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 9:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 10:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 11:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 12:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 13:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 14:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 15:\n",
      "comp sci rec talk articles misc text cmu science computer\n",
      "Topic 16:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 17:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 18:\n",
      "www hardware donated edu educational electronics file files following forsale\n",
      "Topic 19:\n",
      "www hardware donated edu educational electronics file files following forsale\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize and lowercase the text\n",
    "    words = text.lower().split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"Number of preprocessed documents: {len(preprocessed_documents)}\")\n",
    "\n",
    "# Adjust the parameters\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words in each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2862b1-bf26-4e95-88d3-933bda466d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed documents: 1\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of preprocessed documents\n",
    "print(f\"Number of preprocessed documents: {len(preprocessed_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02849ecf-82e4-4908-951a-90a52eea7534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 is in cluster 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert the preprocessed documents to TF-IDF feature vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Number of documents\n",
    "n_documents = tfidf_matrix.shape[0]\n",
    "\n",
    "# Set number of clusters\n",
    "n_clusters = min(20, n_documents)\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Display the cluster labels\n",
    "labels = kmeans.labels_\n",
    "for i, label in enumerate(labels[:10]):  # Display first 10 documents' clusters\n",
    "    print(f\"Document {i} is in cluster {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e817cf-0edb-4555-a98a-9de08498238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix shape: (1, 119)\n",
      "Topic 0:\n",
      "comp sci talk articles rec misc text cmu science computer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Adjust the parameters and preprocess the documents\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=n_clusters, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Display the top words in each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ade0a2-ff0c-4576-9395-6abfe63daf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed documents: 2\n",
      "TF-IDF matrix shape: (2, 3)\n",
      "Document 0 is in cluster 0\n",
      "Document 1 is in cluster 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91/2338014265.py:34: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(tfidf_matrix)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize and lowercase the text\n",
    "    words = text.lower().split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Replace this with your list of documents\n",
    "documents = [\n",
    "    \"Sample document 1 text.\",\n",
    "    \"Sample document 2 text.\",\n",
    "    # Add all your documents here\n",
    "]\n",
    "\n",
    "preprocessed_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"Number of preprocessed documents: {len(preprocessed_documents)}\")\n",
    "\n",
    "# Adjust the parameters\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Set the number of clusters to be the minimum of 20 or the number of documents\n",
    "n_clusters = min(20, tfidf_matrix.shape[0])\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Display the cluster labels\n",
    "labels = kmeans.labels_\n",
    "for i, label in enumerate(labels):  # Display all documents' clusters\n",
    "    print(f\"Document {i} is in cluster {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736cb9ee-6692-48c4-9703-db2af6ecd0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
